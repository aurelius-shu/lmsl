{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python 文本分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字符串操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去空格及特殊符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, world!\n",
      "world!\n",
      " hello, world\n"
     ]
    }
   ],
   "source": [
    "s = ' hello, world!'\n",
    "print s.strip()\n",
    "print s.lstrip(' hello, ')\n",
    "print s.rstrip('!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 连接字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strcatappend\n"
     ]
    }
   ],
   "source": [
    "sStr1 = 'strcat'\n",
    "sStr2 = 'append'\n",
    "sStr1 += sStr2\n",
    "print sStr1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查找字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# < 0 为未找到\n",
    "sStr1 = 'strchr'\n",
    "sStr2 = 'r'\n",
    "nPos = sStr1.index(sStr2)\n",
    "print nPos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比较字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "sStr1 = 'strchr'\n",
    "sStr2 = 'strch'\n",
    "print cmp(sStr2,sStr1)\n",
    "print cmp(sStr1,sStr2)\n",
    "print cmp(sStr1,sStr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字符串中的大小写转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JCSTRLWR\n"
     ]
    }
   ],
   "source": [
    "sStr1 = 'JCstrlwr'\n",
    "sStr1 = sStr1.upper()\n",
    "#sStr1 = sStr1.lower()\n",
    "print sStr1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻转字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfedcba\n"
     ]
    }
   ],
   "source": [
    "sStr1 = 'abcdefg'\n",
    "sStr1 = sStr1[::-1]\n",
    "print sStr1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查找字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "sStr1 = 'abcdefg'\n",
    "sStr2 = 'cde'\n",
    "print sStr1.find(sStr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分割字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cde,fgh,ijk\n",
      "['ab', 'cde', 'fgh', 'ijk']\n"
     ]
    }
   ],
   "source": [
    "sStr1 = 'ab,cde,fgh,ijk'\n",
    "sStr2 = ','\n",
    "sStr1 = sStr1[sStr1.find(sStr2) + 1:]\n",
    "print sStr1\n",
    "#或者\n",
    "s = 'ab,cde,fgh,ijk'\n",
    "print(s.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算字符串中出现频次最多的字幕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#version 1\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def get_max_value_v1(text):\n",
    "    text = text.lower()\n",
    "    result = re.findall('[a-zA-Z]', text)  # 去掉列表中的符号符\n",
    "    count = Counter(result)  # Counter({'l': 3, 'o': 2, 'd': 1, 'h': 1, 'r': 1, 'e': 1, 'w': 1})\n",
    "    count_list = list(count.values())\n",
    "    max_value = max(count_list)\n",
    "    max_list = []\n",
    "    for k, v in count.items():\n",
    "        if v == max_value:\n",
    "            max_list.append(k)\n",
    "    max_list = sorted(max_list)\n",
    "    return max_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#version 2\n",
    "from collections import Counter\n",
    "\n",
    "def get_max_value(text):\n",
    "    count = Counter([x for x in text.lower() if x.isalpha()])\n",
    "    m = max(count.values())\n",
    "    return sorted([x for (x, y) in count.items() if y == m])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#version 3\n",
    "import string\n",
    "\n",
    "def get_max_value(text):\n",
    "    text = text.lower()\n",
    "    return max(string.ascii_lowercase, key=text.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(range(6), key = lambda x : x>2)\n",
    "# >>> 3\n",
    "# 带入key函数中，各个元素返回布尔值，相当于[False, False, False, True, True, True]\n",
    "# key函数要求返回值为True，有多个符合的值，则挑选第一个。\n",
    "\n",
    "max([3,5,2,1,4,3,0], key = lambda x : x)\n",
    "# >>> 5\n",
    "# 带入key函数中，各个元素返回自身的值，最大的值为5，返回5.\n",
    "\n",
    "max('ah', 'bf', key=lambda x: x[1])\n",
    "# >>> 'ah'\n",
    "# 带入key函数，各个字符串返回最后一个字符，其中'ah'的h要大于'bf'中的f，因此返回'ah'\n",
    "\n",
    "max('ah', 'bf', key=lambda x: x[0])\n",
    "# >>> 'bf'\n",
    "# 带入key函数，各个字符串返回第一个字符，其中'bf'的b要大于'ah'中的a，因此返回'bf'\n",
    "\n",
    "text = 'Hello World'\n",
    "max('abcdefghijklmnopqrstuvwxyz', key=text.count)\n",
    "# >>> 'l'\n",
    "# 带入key函数，返回各个字符在'Hello World'中出现的次数，出现次数最多的字符为'l',因此输出'l'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count occurrence of a character in a Python string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "#T  h  e     M  i  s  s  i  s  s  i  p  p  i     R  i  v  e  r\n",
    "#[1, 1, 2, 2, 1, 5, 4, 4, 5, 4, 4, 5, 2, 2, 5, 2, 1, 5, 1, 2, 1]\n",
    "sentence='The Mississippi River'\n",
    "\n",
    "def count_chars(s):\n",
    "        s=s.lower()\n",
    "        count=list(map(s.count,s))\n",
    "        return (max(count))\n",
    "\n",
    "print count_chars(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 正则表达式是**处理字符串**的强大工具，拥有独特的语法和独立的处理引擎。\n",
    "- 在大文本中匹配字符串时，有些情况用str自带的函数(比如find, in)可能可以完成，有些情况会稍稍复杂一些(比如说找出所有“像邮箱”的字符串)，这个时候我们需要一个某种模式的工具，这个时候正则表达式就派上用场了。\n",
    "- 正则表达式效率上可能不如str自带的方法，但匹配功能强大得多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![regular-expression-doc](images/lesson-6/regular-expression-doc.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [在线验证工具](http://regexr.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战与提升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [正则表达式进阶练习](https://alf.nu/RegexGolf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python案例 - re模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用re的一般步骤:\n",
    "- 1.将正则表达式的字符串形式编译为Pattern实例\n",
    "- 2.使用Pattern实例处理文本并获得匹配结果（一个Match实例）\n",
    "- 3.使用Match实例获得信息，进行其他的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, How are you?\n"
     ]
    }
   ],
   "source": [
    "# encoding: UTF-8\n",
    "import re\n",
    " \n",
    "# 将正则表达式编译成Pattern对象\n",
    "pattern = re.compile(r'hello.*\\?')\n",
    " \n",
    "# 使用Pattern匹配文本，获得匹配结果，无法匹配时将返回None\n",
    "match = pattern.match('hello, How are you? And you？')\n",
    " \n",
    "if match:\n",
    "    # 使用Match获得分组信息\n",
    "    print(match.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re.compile(strPattern[, flag]):\n",
    "- Pattern类的工厂方法，用于将字符串形式的正则表达式编译为Pattern对象\n",
    "- 参数flag是匹配模式，取值可以使用按位或运算符'|'表示同时生效，比如re.I | re.M。\n",
    "- 也可以在regex字符串中指定模式，比如re.compile('pattern', re.I | re.M)等价于re.compile('(?im)pattern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flag可选值有：\n",
    "- re.I(re.IGNORECASE): 忽略大小写（括号内是完整写法，下同）\n",
    "- re.M(MULTILINE): 多行模式，改变'^'和'$'的行为（参见上图）\n",
    "- re.S(DOTALL): 点任意匹配模式，改变'.'的行为\n",
    "- re.L(LOCALE): 使预定字符类 \\w \\W \\b \\B \\s \\S 取决于当前区域设定\n",
    "- re.U(UNICODE): 使预定字符类 \\w \\W \\b \\B \\s \\S \\d \\D 取决于unicode定义的字符属性\n",
    "- re.X(VERBOSE): 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。以下两个正则表达式是等价的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_1 = re.compile(r\"\"\"\\d +  # 数字部分\n",
    "                         \\.    # 小数点部分\n",
    "                         \\d *  # 小数的数字部分\"\"\", re.X)\n",
    "regex_2 = re.compile(r\"\\d+\\.\\d*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Match对象是一次匹配的结果，包含了很多关于此次匹配的信息，可以使用Match提供的可读属性或方法来获取这些信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match属性：\n",
    "- string: 匹配时使用的文本。\n",
    "- re: 匹配时使用的Pattern对象。\n",
    "- pos: 文本中正则表达式开始搜索的索引。值与Pattern.match()和Pattern.seach()方法的同名参数相同。\n",
    "- endpos: 文本中正则表达式结束搜索的索引。值与Pattern.match()和Pattern.seach()方法的同名参数相同。\n",
    "- lastindex: 最后一个被捕获的分组在文本中的索引。如果没有被捕获的分组，将为None。\n",
    "- lastgroup: 最后一个被捕获的分组的别名。如果这个分组没有别名或者没有被捕获的分组，将为None。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法：\n",
    "- group([group1, …]): \n",
    "\n",
    "    获得一个或多个分组截获的字符串；指定多个参数时将以元组形式返回。group1可以使用编号也可以使用别名；编号0代表整个匹配的子串；不填写参数时，返回group(0)；没有截获字符串的组返回None；截获了多次的组返回最后一次截获的子串。\n",
    "    \n",
    "- groups([default]): \n",
    "    \n",
    "    以元组形式返回全部分组截获的字符串。相当于调用group(1,2,…last)。default表示没有截获字符串的组以这个值替代，默认为None。\n",
    "    \n",
    "- groupdict([default]): \n",
    "    \n",
    "    返回以有别名的组的别名为键、以该组截获的子串为值的字典，没有别名的组不包含在内。default含义同上。\n",
    "    \n",
    "- start([group]): \n",
    "    \n",
    "    返回指定的组截获的子串在string中的起始索引（子串第一个字符的索引）。group默认值为0。\n",
    "    \n",
    "- end([group]): \n",
    "    \n",
    "    返回指定的组截获的子串在string中的结束索引（子串最后一个字符的索引+1）。group默认值为0。\n",
    "    \n",
    "- span([group]): \n",
    "    \n",
    "    返回(start(group), end(group))。\n",
    "    \n",
    "- expand(template): \n",
    "    \n",
    "    将匹配到的分组代入template中然后返回。template中可以使用\\id或\\g、\\g引用分组，但不能使用编号0。\\id与\\g是等价的；但\\10将被认为是第10个分组，如果你想表达\\1之后是字符'0'，只能使用\\g<1>0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m.string: hello hanxiaoyang!\n",
      "m.re: re.compile('(\\\\w+) (\\\\w+)(?P<sign>.*)')\n",
      "m.pos: 0\n",
      "m.endpos: 18\n",
      "m.lastindex: 3\n",
      "m.lastgroup: sign\n",
      "m.group(1,2): ('hello', 'hanxiaoyang')\n",
      "m.groups(): ('hello', 'hanxiaoyang', '!')\n",
      "m.groupdict(): {'sign': '!'}\n",
      "m.start(2): 6\n",
      "m.end(2): 17\n",
      "m.span(2): (6, 17)\n",
      "m.expand(r'\\2 \\1\\3'): hanxiaoyang hello!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "m = re.match(r'(\\w+) (\\w+)(?P<sign>.*)', 'hello hanxiaoyang!')\n",
    " \n",
    "print(\"m.string:\", m.string)\n",
    "print (\"m.re:\", m.re)\n",
    "print (\"m.pos:\", m.pos)\n",
    "print (\"m.endpos:\", m.endpos)\n",
    "print (\"m.lastindex:\", m.lastindex)\n",
    "print (\"m.lastgroup:\", m.lastgroup)\n",
    " \n",
    "print (\"m.group(1,2):\", m.group(1, 2))\n",
    "print( \"m.groups():\", m.groups())\n",
    "print( \"m.groupdict():\", m.groupdict())\n",
    "print( \"m.start(2):\", m.start(2))\n",
    "print( \"m.end(2):\", m.end(2))\n",
    "print( \"m.span(2):\", m.span(2))\n",
    "print( r\"m.expand(r'\\2 \\1\\3'):\", m.expand(r'\\2 \\1\\3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pattern对象是一个编译好的正则表达式，通过Pattern提供的一系列方法可以对文本进行匹配查找。\n",
    "- Pattern不能直接实例化，必须使用re.compile()进行构造。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pattern提供了几个可读属性用于获取表达式的相关信息：\n",
    "\n",
    "- pattern: 编译时用的表达式字符串。\n",
    "- flags: 编译时用的匹配模式。数字形式。\n",
    "- groups: 表达式中分组的数量。\n",
    "- groupindex: 以表达式中有别名的组的别名为键、以该组对应的编号为值的字典，没有别名的组不包含在内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p.pattern: (\\w+) (\\w+)(?P<sign>.*)\n",
      "p.flags: 48\n",
      "p.groups: 3\n",
      "p.groupindex: {'sign': 3}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile(r'(\\w+) (\\w+)(?P<sign>.*)', re.DOTALL)\n",
    " \n",
    "print(\"p.pattern:\", p.pattern)\n",
    "print(\"p.flags:\", p.flags)\n",
    "print(\"p.groups:\", p.groups)\n",
    "print(\"p.groupindex:\", p.groupindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用pattern**\n",
    "- match(string[, pos[, endpos]]) | re.match(pattern, string[, flags]):     \n",
    "    这个方法将从string的pos下标处起尝试匹配pattern:\n",
    "    - 如果pattern结束时仍可匹配，则返回一个Match对象\n",
    "    - 如果匹配过程中pattern无法匹配，或者匹配未结束就已到达endpos，则返回None。\n",
    "    - pos和endpos的默认值分别为0和len(string)。 \n",
    "    ** 注意：这个方法并不是完全匹配。当pattern结束时若string还有剩余字符，仍然视为成功。想要完全匹配，可以在表达式末尾加上边界匹配符'$'。**\n",
    "    \n",
    "    \n",
    "- search(string[, pos[, endpos]]) | re.search(pattern, string[, flags]):     \n",
    "    这个方法从string的pos下标处起尝试匹配pattern\n",
    "    - 如果pattern结束时仍可匹配，则返回一个Match对象\n",
    "    - 若无法匹配，则将pos加1后重新尝试匹配，直到pos=endpos时仍无法匹配则返回None。\n",
    "    - pos和endpos的默认值分别为0和len(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World\n"
     ]
    }
   ],
   "source": [
    "# encoding: UTF-8 \n",
    "import re \n",
    " \n",
    "# 将正则表达式编译成Pattern对象 \n",
    "pattern = re.compile(r'W.*d') \n",
    " \n",
    "# 使用search()查找匹配的子串，不存在能匹配的子串时将返回None \n",
    "# 这个例子中使用match()无法成功匹配 \n",
    "match = pattern.search('hello World!') \n",
    " \n",
    "if match: \n",
    "    # 使用Match获得分组信息 \n",
    "    print(match.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split(string[, maxsplit]) | re.split(pattern, string[, maxsplit]):\n",
    "- 按照能够匹配的子串将string分割后返回列表。\n",
    "- maxsplit用于指定最大分割次数，不指定将全部分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three', 'four', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "p = re.compile(r'\\d+')\n",
    "print(p.split('one1two2three3four4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### findall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "findall(string[, pos[, endpos]]) | re.findall(pattern, string[, flags]):\n",
    "- 搜索string，以列表形式返回全部能匹配的子串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "p = re.compile(r'\\d+')\n",
    "print(p.findall('one1two2three3four4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### finditer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finditer(string[, pos[, endpos]]) | re.finditer(pattern, string[, flags]):\n",
    "- 搜索string，返回一个顺序访问每一个匹配结果（Match对象）的迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "p = re.compile(r'\\d+')\n",
    "for m in p.finditer('one1two2three3four4'):\n",
    "    print(m.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sub(repl, string[, count]) | re.sub(pattern, repl, string[, count]):\n",
    "- 使用repl替换string中每一个匹配的子串后返回替换后的字符串。\n",
    "    - 当repl是一个字符串时，可以使用\\id或\\g、\\g引用分组，但不能使用编号0。\n",
    "    - 当repl是一个方法时，这个方法应当只接受一个参数（Match对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。 count用于指定最多替换次数，不指定时全部替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say i, hanxiaoyang hello!\n",
      "I Say, Hello Hanxiaoyang!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "p = re.compile(r'(\\w+) (\\w+)')\n",
    "s = 'i say, hello hanxiaoyang!'\n",
    " \n",
    "print(p.sub(r'\\2 \\1', s))\n",
    " \n",
    "def func(m):\n",
    "    return m.group(1).title() + ' ' + m.group(2).title()\n",
    " \n",
    "print(p.sub(func, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### subn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subn(repl, string[, count]) |re.sub(pattern, repl, string[, count]):\n",
    "- 返回 (sub(repl, string[, count]), 替换次数)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('say i, hanxiaoyang hello!', 2)\n",
      "('I Say, Hello Hanxiaoyang!', 2)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "p = re.compile(r'(\\w+) (\\w+)')\n",
    "s = 'i say, hello hanxiaoyang!'\n",
    " \n",
    "print(p.subn(r'\\2 \\1', s))\n",
    " \n",
    "def func(m):\n",
    "    return m.group(1).title() + ' ' + m.group(2).title()\n",
    " \n",
    "print(p.subn(func, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba 中文处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和拉丁语系不同，亚洲语言是不用空格分开每个有意义的词的。而当我们进行自然语言处理的时候，大部分情况下，词汇是我们对句子和文章理解的基础，因此需要一个工具去把完整的文本中分解成粒度更细的词。\n",
    "\n",
    "jieba就是这样一个非常好用的中文工具，是以分词起家的，但是功能比分词要强大很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**jieba.cut** 方法接受三个输入参数:\n",
    "- 需要分词的字符串\n",
    "- cut_all 参数用来控制是否采用全模式\n",
    "- HMM 参数用来控制是否使用 HMM 模型\n",
    "\n",
    "**jieba.cut_for_search** 方法接受两个参数\n",
    "- 需要分词的字符串\n",
    "- 是否使用 HMM 模型。\n",
    "\n",
    "该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Tokenizer.cut at 0x0000013E779099A8>\n",
      "Full Mode: 我/ 在/ 学习/ 自然/ 自然语言/ 语言/ 处理\n",
      "Default Mode: 我/ 在/ 学习/ 自然语言/ 处理\n",
      "他, 毕业, 于, 上海交通大学, ，, 在, 百度, 深度, 学习, 研究院, 进行, 研究\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 哈佛, 大学, 哈佛大学, 深造\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"我在学习自然语言处理\", cut_all=True)\n",
    "print(seg_list)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我在学习自然语言处理\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"他毕业于上海交通大学，在百度深度学习研究院进行研究\")  # 默认是精确模式\n",
    "print(\", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在哈佛大学深造\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 直接返回 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['小明', '硕士', '毕业', '于', '中国科学院', '计算所', '，', '后', '在', '哈佛大学', '深造']\n",
      "小明 硕士 毕业 于 中国科学院 计算所 ， 后 在 哈佛大学 深造\n",
      "小明 硕士 毕业 于 中国 科学 学院 科学院 中国科学院 计算 计算所 ， 后 在 哈佛 大学 哈佛大学 深造\n"
     ]
    }
   ],
   "source": [
    "result_lcut = jieba.lcut(\"小明硕士毕业于中国科学院计算所，后在哈佛大学深造\")\n",
    "print(result_lcut)\n",
    "print(\" \".join(result_lcut))\n",
    "print(\" \".join(jieba.lcut_for_search(\"小明硕士毕业于中国科学院计算所，后在哈佛大学深造\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 添加用户自定义词典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很多时候我们需要针对自己的场景进行分词，会有一些领域内的专有词汇。\n",
    "- 1.可以用jieba.load_userdict(file_name)加载用户字典\n",
    "- 2.少量的词汇可以自己用下面方法手动添加：\n",
    "    - 用 add_word(word, freq=None, tag=None) 和 del_word(word) 在程序中动态修改词典\n",
    "    - 用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其能（或不能）被分出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果/放到/旧/字典/中将/出错/。\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('如果放到旧字典中将出错。', HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.suggest_freq(('中', '将'), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果/放到/旧/字典/中/将/出错/。\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('如果放到旧字典中将出错。', HMM=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基于 TF-IDF 算法的关键词抽取**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())\n",
    "- sentence 为待提取的文本\n",
    "- topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n",
    "- withWeight 为是否一并返回关键词权重值，默认值为 False\n",
    "- allowPOS 仅包括指定词性的词，默认值为空，即不筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "韦少  杜兰特  全明星  全明星赛  MVP  威少  正赛  科尔  投篮  勇士  球员  斯布鲁克  更衣柜  NBA  三连庄  张卫平  西部  指导  雷霆  明星队\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "lines = open('data/lesson-6/NBA.txt', encoding='utf-8').read()\n",
    "print(\"  \".join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行者  八戒  师父  三藏  唐僧  大圣  沙僧  妖精  菩萨  和尚  那怪  那里  长老  呆子  徒弟  怎么  不知  老孙  国王  一个\n"
     ]
    }
   ],
   "source": [
    "lines = open(u'data/lesson-6/西游记.txt', encoding='GB2312', errors='ignore').read()\n",
    "print(\"  \".join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**关于TF-IDF 算法的关键词抽取补充**\n",
    "关键词提取所使用逆向文件频率（IDF）文本语料库可以切换成自定义语料库的路径\n",
    "- 用法： jieba.analyse.set_idf_path(file_name) # file_name为自定义语料库的路径\n",
    "    - 自定义语料库示例见这里\n",
    "    - 用法示例见这里\n",
    "- 关键词提取所使用停止词（Stop Words）文本语料库可以切换成自定义语料库的路径\n",
    "    - 用法： jieba.analyse.set_stop_words(file_name) # file_name为自定义语料库的路径\n",
    "    - 自定义语料库示例见这里\n",
    "    - 用法示例见这里\n",
    "\n",
    "关键词一并返回关键词权重值示例\n",
    "- 用法示例见这里"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基于 TextRank 算法的关键词抽取**\n",
    "- jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。\n",
    "- jieba.analyse.TextRank() 新建自定义 TextRank 实例\n",
    "\n",
    "算法论文：[TextRank: Bringing Order into Texts](http://120.52.51.16/web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n",
    "\n",
    "基本思想:\n",
    "- 将待抽取关键词的文本进行分词\n",
    "- 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图\n",
    "- 计算图中节点的PageRank，注意是无向带权图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全明星赛  勇士  正赛  指导  对方  投篮  球员  没有  出现  时间  威少  认为  看来  结果  相隔  助攻  现场  三连庄  介绍  嘉宾\n",
      "---------------------我是分割线----------------\n",
      "勇士  正赛  全明星赛  指导  投篮  玩命  时间  对方  现场  结果  球员  嘉宾  时候  全队  主持人  照片  全程  目标  快船队  肥皂剧\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "lines = open('data/lesson-6/NBA.txt', encoding='utf-8').read()\n",
    "print(\"  \".join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))))\n",
    "print(\"---------------------我是分割线----------------\")\n",
    "print(\"  \".join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行者  师父  八戒  三藏  大圣  不知  菩萨  妖精  只见  长老  国王  却说  呆子  徒弟  小妖  出来  不得  不见  不能  师徒\n"
     ]
    }
   ],
   "source": [
    "lines = open(u'data/lesson-6/西游记.txt', errors='ignore').read()\n",
    "print(\"  \".join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。\n",
    "- 标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。\n",
    "- 具体的词性对照表参见[计算所汉语词性标记集](http://ictclas.nlpir.org/nlpir/html/readme.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "爱 v\n",
      "自然语言 l\n",
      "处理 v\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我爱自然语言处理\")\n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 并行分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升 基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows\n",
    "\n",
    "用法：\n",
    "\n",
    "    jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数\n",
    "    jieba.disable_parallel() # 关闭并行分词模式\n",
    "    \n",
    "实验结果：在 4 核 3.4GHz Linux 机器上，对金庸全集进行精确分词，获得了 1MB/s 的速度，是单进程版的 3.3 倍。\n",
    "\n",
    "注意：并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "jieba: parallel mode only supports posix system",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-e9f4d1a7aab6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'data/lesson-6/西游记.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\vital\\notebook\\venv\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36menable_parallel\u001b[1;34m(processnum)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nt'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         raise NotImplementedError(\n\u001b[1;32m--> 578\u001b[1;33m             \"jieba: parallel mode only supports posix system\")\n\u001b[0m\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: jieba: parallel mode only supports posix system"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import jieba\n",
    "\n",
    "jieba.enable_parallel()\n",
    "content = open(u'data/lesson-6/西游记.txt',\"r\").read()\n",
    "t1 = time.time()\n",
    "words = \"/ \".join(jieba.cut(content))\n",
    "t2 = time.time()\n",
    "tm_cost = t2-t1\n",
    "print('并行分词速度为 %s bytes/second' % (len(content)/tm_cost))\n",
    "\n",
    "jieba.disable_parallel()\n",
    "content = open(u'data/lesson-6/西游记.txt',\"r\").read()\n",
    "t1 = time.time()\n",
    "words = \"/ \".join(jieba.cut(content))\n",
    "t2 = time.time()\n",
    "tm_cost = t2-t1\n",
    "print('非并行分词速度为 %s bytes/second' % (len(content)/tm_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**返回词语在原文的起止位置**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是默认模式的tokenize\n",
      "自然语言\t\t start: 0 \t\t end:4\n",
      "处理\t\t start: 4 \t\t end:6\n",
      "非常\t\t start: 6 \t\t end:8\n",
      "有用\t\t start: 8 \t\t end:10\n",
      "\n",
      "-----------我是神奇的分割线------------\n",
      "\n",
      "这是搜索模式的tokenize\n",
      "自然\t\t start: 0 \t\t end:2\n",
      "语言\t\t start: 2 \t\t end:4\n",
      "自然语言\t\t start: 0 \t\t end:4\n",
      "处理\t\t start: 4 \t\t end:6\n",
      "非常\t\t start: 6 \t\t end:8\n",
      "有用\t\t start: 8 \t\t end:10\n"
     ]
    }
   ],
   "source": [
    "# 输入参数只接受 unicode\n",
    "print(\"这是默认模式的tokenize\")\n",
    "result = jieba.tokenize(u'自然语言处理非常有用')\n",
    "for tk in result:\n",
    "    print(\"%s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))\n",
    "\n",
    "print(\"\\n-----------我是神奇的分割线------------\\n\")\n",
    "\n",
    "print(\"这是搜索模式的tokenize\")\n",
    "result = jieba.tokenize(u'自然语言处理非常有用', mode='search')\n",
    "for tk in result:\n",
    "    print(\"%s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChineseAnalyzer for Whoosh 搜索引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'jieba.analyse' has no attribute 'ChineseAnalyzer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-9e2b97f7e4c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwhoosh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqparser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQueryParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0manalyzer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChineseAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstored\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstored\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstored\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'jieba.analyse' has no attribute 'ChineseAnalyzer'"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "from whoosh.index import create_in,open_dir\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "analyzer = jieba.analyse.ChineseAnalyzer()\n",
    "schema = Schema(title=TEXT(stored=True), path=ID(stored=True), content=TEXT(stored=True, analyzer=analyzer))\n",
    "    \n",
    "if not os.path.exists(\"tmp\"):\n",
    "    os.mkdir(\"tmp\")\n",
    "\n",
    "ix = create_in(\"tmp\", schema) # for create new index\n",
    "#ix = open_dir(\"tmp\") # for read only\n",
    "writer = ix.writer()\n",
    "\n",
    "writer.add_document(\n",
    "    title=\"document1\",\n",
    "    path=\"/a\",\n",
    "    content=\"This is the first document we’ve added!\"\n",
    ")\n",
    "\n",
    "writer.add_document(\n",
    "    title=\"document2\",\n",
    "    path=\"/b\",\n",
    "    content=\"The second one 你 中文测试中文 is even more interesting! 吃水果\"\n",
    ")\n",
    "\n",
    "writer.add_document(\n",
    "    title=\"document3\",\n",
    "    path=\"/c\",\n",
    "    content=\"买水果然后来世博园。\"\n",
    ")\n",
    "\n",
    "writer.add_document(\n",
    "    title=\"document4\",\n",
    "    path=\"/c\",\n",
    "    content=\"工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作\"\n",
    ")\n",
    "\n",
    "writer.add_document(\n",
    "    title=\"document4\",\n",
    "    path=\"/c\",\n",
    "    content=\"咱俩交换一下吧。\"\n",
    ")\n",
    "\n",
    "writer.commit()\n",
    "searcher = ix.searcher()\n",
    "parser = QueryParser(\"content\", schema=ix.schema)\n",
    "\n",
    "for keyword in (\"水果世博园\",\"你\",\"first\",\"中文\",\"交换机\",\"交换\"):\n",
    "    print(keyword+\"的结果为如下：\")\n",
    "    q = parser.parse(keyword)\n",
    "    results = searcher.search(q)\n",
    "    for hit in results:\n",
    "        print(hit.highlights(\"content\"))\n",
    "    print(\"\\n--------------我是神奇的分割线--------------\\n\")\n",
    "\n",
    "for t in analyzer(\"我的好朋友是李明;我爱北京天安门;IBM和Microsoft; I have a dream. this is intetesting and interested me a lot\"):\n",
    "    print(t.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命令行分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` txt\n",
    "使用示例：python -m jieba news.txt > cut_result.txt\n",
    "\n",
    "命令行选项（翻译）：\n",
    "\n",
    "使用: python -m jieba [options] filename\n",
    "\n",
    "结巴命令行界面。\n",
    "\n",
    "固定参数:\n",
    "  filename              输入文件\n",
    "\n",
    "可选参数:\n",
    "  -h, --help            显示此帮助信息并退出\n",
    "  -d [DELIM], --delimiter [DELIM]\n",
    "                        使用 DELIM 分隔词语，而不是用默认的' / '。\n",
    "                        若不指定 DELIM，则使用一个空格分隔。\n",
    "  -p [DELIM], --pos [DELIM]\n",
    "                        启用词性标注；如果指定 DELIM，词语和词性之间\n",
    "                        用它分隔，否则用 _ 分隔\n",
    "  -D DICT, --dict DICT  使用 DICT 代替默认词典\n",
    "  -u USER_DICT, --user-dict USER_DICT\n",
    "                        使用 USER_DICT 作为附加词典，与默认词典或自定义词典配合使用\n",
    "  -a, --cut-all         全模式分词（不支持词性标注）\n",
    "  -n, --no-hmm          不使用隐含马尔可夫模型\n",
    "  -q, --quiet           不输出载入信息到 STDERR\n",
    "  -V, --version         显示版本信息并退出\n",
    "\n",
    "如果没有指定文件名，则使用标准输入。\n",
    "\n",
    "--help 选项输出：\n",
    "\n",
    "$> python -m jieba --help\n",
    "Jieba command line interface.\n",
    "\n",
    "positional arguments:\n",
    "  filename              input file\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -d [DELIM], --delimiter [DELIM]\n",
    "                        use DELIM instead of ' / ' for word delimiter; or a\n",
    "                        space if it is used without DELIM\n",
    "  -p [DELIM], --pos [DELIM]\n",
    "                        enable POS tagging; if DELIM is specified, use DELIM\n",
    "                        instead of '_' for POS delimiter\n",
    "  -D DICT, --dict DICT  use DICT as dictionary\n",
    "  -u USER_DICT, --user-dict USER_DICT\n",
    "                        use USER_DICT together with the default dictionary or\n",
    "                        DICT (if specified)\n",
    "  -a, --cut-all         full pattern cutting (ignored with POS tagging)\n",
    "  -n, --no-hmm          don't use the Hidden Markov Model\n",
    "  -q, --quiet           don't print loading messages to stderr\n",
    "  -V, --version         show program's version number and exit\n",
    "\n",
    "If no filename specified, use STDIN instead.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
