{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIVE 系统架构\n",
    "```\n",
    "    建立在 Hadoop 上的数据仓库基础架构，ETL 的一种，定义语法 HQL 是一种 SQL 解析引擎，将 SQL 语句转译成 MapReduce Job。\n",
    "    Hive 表对应 HDFS 上的文件夹，数据对应 HDFS 上的文件。\n",
    "```\n",
    "***\n",
    "- 用户接口： CLI, JDBC/ODBC, WebUI\n",
    "- 元数据存储： mysql, derby\n",
    "- 解释器、编译器、优化器、执行器\n",
    "- Hadoop 的 HDFS 存储， MR 的计算\n",
    "***\n",
    "\n",
    "## 配置\n",
    "``` bash\n",
    "cd ../conf\n",
    "mv hive-default.xml.template hive-site.xml\n",
    "vi hive-site.xml\n",
    "```\n",
    "\n",
    "``` xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionURL</name>\n",
    "        <value>jdbc:mysql://ip:3306/hive?createDatabaseIfNotExist=true</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionDriverName</name>\n",
    "        <value>com.mysql.jdbc.Driver</value>\n",
    "    </property>\n",
    "        <name>javax.jdo.option.ConnectionUserName</name>\n",
    "        <value>root</value>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionPassword</name>\n",
    "        <value>123</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "``` bash\n",
    "# 进入 hive\n",
    "bin/hive\n",
    "```\n",
    "\n",
    "``` bash\n",
    "# 无密码进入 beeline\n",
    "# hadoop 修改配置\n",
    "vi core-site.xml\n",
    "```\n",
    "```\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.aurelius.hosts</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.aurelius.groups</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "```\n",
    "``` bash\n",
    "# 启动元数据服务\n",
    "bin/hive --service metastore & \n",
    "# 启动 hiveserver2\n",
    "bin/hiveserver2\n",
    "bin/beeline\n",
    "beeline> !connect jdbc:hive2://ip:10000\n",
    "```\n",
    "\n",
    "## 语法\n",
    "``` sql\n",
    "-- 创建表\n",
    "create table teacher(id bigint, name string) row format delimited fields terminated by '\\t';\n",
    "-- 创建外部表\n",
    "create external table beauties(id  bigint, name string) row format delimited fields terminated by '\\t';\n",
    "-- 创建分区表\n",
    "create external table beauties(id bigint, name string) partitioned by (nation string) row format delimited fields terminated by '\\t';\n",
    "-- 查看创建表\n",
    "show create table teacher;\n",
    "-- 导入数据\n",
    "load data local inpath '/root/1.txt' into table teacher;\n",
    "load data local inpath '/root/a/x' into table beauties partition(nation='China');\n",
    "-- 修改表分区\n",
    "-- 在 hdfs手动在表路径建的路径不会自动添加到hive 表分区，须通过alter 语句修改 metastore 中元数据\n",
    "alter table beauties add partition(nation='Japan' location \"/beauty/nation=Japan\";\n",
    "-- 查看数据\n",
    "select * from teacher;\n",
    "-- 创建库\n",
    "create database db1;\n",
    "```\n",
    "\n",
    "## sqoop 与 hive\n",
    "``` bash\n",
    "./sqoop import --connect jdbc:mysql://ip:3306/db \\\n",
    "--username root --password 123 \\\n",
    "--table beauties --hive-import \\\n",
    "--hive-overwrite --hive-table beauties \\\n",
    "--fields-terminated-by '\\t';\n",
    "```\n",
    "\n",
    "## hive udf\n",
    "### udf demo\n",
    "``` java\n",
    "package hive.udf;\n",
    "\n",
    "import org.apache.hadoop.hive.ql.exec.UDF;\n",
    "import org.apache.hadoop.io.Text;\n",
    "\n",
    "public class UdfDemo extends UDF {\n",
    "    public Text evaluate(Text input) {\n",
    "        String na_e = input.toString();\n",
    "        String na = naMap.get(na_e);\n",
    "        if (na == null) {\n",
    "            na = \"test\";\n",
    "        }\n",
    "        t.set(na);\n",
    "        return t;\n",
    "    }\n",
    "\n",
    "    Text t = new Text();\n",
    "}\n",
    "```\n",
    "\n",
    "### udf 调用过程\n",
    "``` bash\n",
    "# 1. 添加 jar 包（在hive 中执行，注意 java 的版本）\n",
    "hive> add jar /root/UDFDemo.jar;\n",
    "# 2. 创建临时函数\n",
    "hive> create temporary function funcname as 'com.edu.hive.udf.UDFDemo';\n",
    "# 3. 调用\n",
    "hive> select id, funcname(name) from beauties;\n",
    "# 4. 将查询结果保存到 HDFS\n",
    "hive> create table result row format delimited fields terminated by '\\t' as \\\n",
    "hive> select id, funcname(name) from beauties;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exception\n",
    "### ${路径问题}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
