{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hadoop 概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The Hadoop Ecosystem](./images/the-hadoop-ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hadoop 起源于 Doug Cutting，是行业大数据标准开源软件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用处\n",
    "- 海量数据存储（HDFS）\n",
    "- 海量数据分析（MapReduce）(YARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "版本\n",
    "- Apache ／ Cloudera(CDH) / HDP(Hortonworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核心\n",
    "- HDFS：Hadoop Distributed File System\n",
    "- MapReduce：并行计算框架\n",
    "- YARN：Yet Another Resource Negotiator(资源管理调度系统)\n",
    "    - 可运行 Storm／Spark 等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS 架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主从结构\n",
    "- 主节点：只有一个，namenode\n",
    "- 从节点：有多个，datanodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "namenode\n",
    "- 接收用户操作请求\n",
    "- 维护文件系统的目录结构\n",
    "- 管理文件与 block 之间的关系，block 与 datanode 之间的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datanode\n",
    "- 存储文件\n",
    "- 文件被分成 block 存储在磁盘上\n",
    "- 为保证数据的安全，文件会有多个副本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linux 环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主机名 hostname 修改\n",
    "``` bash\n",
    "vim /etc/sysconfig/network\n",
    "```\n",
    "host 映射修改\n",
    "``` bash\n",
    "vim /etc/hosts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ip 修改\n",
    "``` bash\n",
    "vim /etc/sysconfig/network-scripts/ifcfg-eth0\n",
    "```\n",
    "文件末尾添加：\n",
    "``` txt\n",
    "BOOTPROTO = \"static\"\n",
    "IPADDR = \"192.168.177.1\"\n",
    "NETMASK = \"255.255.255.0\"\n",
    "GATEWAY = \"192.168.177.1\"\n",
    "DNS1 = \"8.8.8.8\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关闭防火墙\n",
    "``` bash\n",
    "service iptables stop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关闭防火墙开机启动\n",
    "``` bash\n",
    "chkconfig iptables off\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三种虚拟网络模式\n",
    "- bridge: 局域网的一台独立主机，手动 TCP/IP\n",
    "- nat: 通过宿主机器所在网络访问公网 VMnet8(NAT), 不用任何手动配置\n",
    "- host-only: 与真实网络隔离 VMnet1， 虚拟网络的 DHCP 服务器动态分配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "vim /etc/profile\n",
    "```\n",
    "或\n",
    "``` bash\n",
    "export JAVA_HOME = /usr/java/jdk1.8.0_181\n",
    "export PATH = $PATH:$JAVA_HOME/bin\n",
    "```\n",
    "刷新配置\n",
    "``` bash\n",
    "source /etc/profile\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hadoop 配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 版本：hadoop-2.9.1\n",
    "- 路径：/hadoop\n",
    "- 配置文件（/hadoop/hadoop-2.9.1/etc/hadoop）：\n",
    "    1. 依赖的jdk:(hadoop-env.sh)\n",
    "    ``` bash\n",
    "    export JAVA_HOME=/usr/java/jdk1.8.0_181\n",
    "    ```\n",
    "    \n",
    "    2. core-site.xml:\n",
    "        ``` xml\n",
    "        <!--单机-->\n",
    "        <!--HDFS的NameNode地址-->\n",
    "        <property>\n",
    "                <name>fs.defaultFS</name>\n",
    "                <value>hdfs://centos-base:9000</value>\n",
    "        </property>\n",
    "        <!--Runtime files-->\n",
    "        <property>\n",
    "                <name>hadoop.tmp.dir</name>\n",
    "                <value>/hadoop/hadoop-2.9.1/tmp</value>\n",
    "        </property>\n",
    "        ```\n",
    "        \n",
    "        ``` xml\n",
    "        <!--集群-->\n",
    "        <!--HDFS NameNode-->\n",
    "        <property>\n",
    "                <name>fs.defaultFS</name>\n",
    "                <value>hdfs://ns1</value>\n",
    "        </property>\n",
    "        <!--runtime files-->\n",
    "        <property>\n",
    "                <name>hadoop.tmp.dir</name>\n",
    "                <value>/hadoop/hadoop-2.9.1/tmp</value>\n",
    "        </property>\n",
    "        <property>\n",
    "                <name>ha.zookeeper.quorum</name>\n",
    "                <value>node04:2181,node05:2181,node06:2181</value>\n",
    "        </property>\n",
    "        ```\n",
    "        \n",
    "    3. hdfs.site.xml\n",
    "        ``` xml\n",
    "        <!--单机-->\n",
    "        <!--hdfs data 保存副本数量-->\n",
    "        <property>\n",
    "                <name>dfs.replication</name>\n",
    "                <value>1</value>\n",
    "        </property>\n",
    "        ```\n",
    "        \n",
    "        ``` xml\n",
    "        <!--集群-->        \n",
    "        <!--这里抽象出两个NameService实际上就是给这个HDFS集群起了个别名-->\n",
    "        <property>\n",
    "            <name>dfs.nameservices</name>\n",
    "            <value>ns1</value>\n",
    "        </property>\n",
    "\n",
    "        <!--指定NameService是cluster1时的namenode有哪些,这里的值也是逻辑名称，名字随便起，相互不重复即可-->\n",
    "        <property>\n",
    "            <name>dfs.ha.namenodes.ns1</name>\n",
    "            <value>nn1,nn2</value>\n",
    "        </property>\n",
    "\n",
    "        <!--指定nn1,nn2的RPC地址 8020 9000-->\n",
    "            <property>\n",
    "                <name>dfs.namenode.rpc-address.ns1.nn1</name>\n",
    "                <value>node01:9000</value>\n",
    "            </property>\n",
    "            <property>\n",
    "                <name>dfs.namenode.rpc-address.ns1.nn2</name>\n",
    "                <value>node02:9000</value>\n",
    "            </property>\n",
    "\n",
    "        <!--指定nn1、nn2的http地址-->\n",
    "            <property>\n",
    "                <name>dfs.namenode.http-address.ns1.nn1</name>\n",
    "                <value>node01:50070</value>\n",
    "            </property>\n",
    "            <property>\n",
    "                <name>dfs.namenode.http-address.ns1.nn2</name>\n",
    "                <value>node02:50070</value>\n",
    "            </property>\n",
    "\n",
    "        <!--指定cluster1的两个NameNode共享edits文件目录时，使用的JournalNode集群信息-->\n",
    "            <property>\n",
    "                <name>dfs.namenode.shared.edits.dir</name>\n",
    "                <value>qjournal://node04:8485;node05:8485;node06:8485/ns1</value>\n",
    "            </property>\n",
    "\n",
    "        <!--指定JournalNode集群在对NameNode的目录进行共享时，自己存储数据的磁盘路径-->\n",
    "            <property>\n",
    "              <name>dfs.journalnode.edits.dir</name>\n",
    "              <value>/hadoop/hadoop-2.9.1/data/jn</value>\n",
    "            </property>\n",
    "\n",
    "        <!--指定是否启动自动故障恢复，即当NameNode出故障时，是否自动切换到另一台NameNode-->\n",
    "            <property>\n",
    "               <name>dfs.ha.automatic-failover.enabled</name>\n",
    "               <value>true</value>\n",
    "             </property>\n",
    "\n",
    "        <!--指定cluster1出故障时，哪个实现类负责执行故障切换-->\n",
    "            <property>\n",
    "              <name>dfs.client.failover.proxy.provider.ns1</name>\n",
    "              <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n",
    "            </property>\n",
    "\n",
    "        <!--一旦需要NameNode切换，使用ssh方式进行操作-->\n",
    "            <property>\n",
    "              <name>dfs.ha.fencing.methods</name>\n",
    "              <value>\n",
    "                sshfence\n",
    "                shell(/bin/true)\n",
    "              </value>\n",
    "            </property>\n",
    "\n",
    "        <!--如果使用ssh进行故障切换，使用ssh通信时用的密钥存储的位置-->\n",
    "            <property>\n",
    "              <name>dfs.ha.fencing.ssh.private-key-files</name>\n",
    "              <value>/home/beifeng/.ssh/id_rsa</value>\n",
    "            </property>\n",
    "\n",
    "        <!--sshfence 超时时间-->\n",
    "            <property>\n",
    "                <name>dfs.ha.fencing.ssh.connect-timeout</name>\n",
    "                <value>30000</value>\n",
    "            </property>\n",
    "\n",
    "        <!--指定DataNode存储block的副本数量-->\n",
    "            <property>\n",
    "                <name>dfs.replication</name>\n",
    "                <value>3</value>\n",
    "            </property>\n",
    "        <!--解释：hadoop 守护进程一般同时运行RPC 和HTTP两个服务器，RPC服务器支持守护进程间的通信，HTTP服务器则提供与用户交互的Web页面。-->\n",
    "        ```\n",
    "        \n",
    "    4. mapred-site.xml\n",
    "        ``` xml\n",
    "        <!--mapreduce运行在yarn上-->\n",
    "        <property>\n",
    "                <name>mapreduce.framework.name</name>\n",
    "                <value>yarn</value>\n",
    "        </property>\n",
    "        ```\n",
    "        \n",
    "    5. yarn-site.xml\n",
    "        ``` xml\n",
    "        <!--制定数据获取方式是shuffle-->\n",
    "        <property>\n",
    "                <name>yarn.nodemanager.aux-services</name>\n",
    "                <value>mapreduce_shuffle</value>\n",
    "        </property>\n",
    "        <!--指定yarn的resourcemanager地址-->\n",
    "        <property>\n",
    "                <name>yarn.resourcemanager.hostname</name>\n",
    "                <value>centos-base</value>\n",
    "        </property>\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "export HADOOP_HOME=/hadoop/hadoop-2.9.1\n",
    "export PATH=$PATH:$HADOOP_HOME/bin\n",
    "source /etc/profile\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hadoop 文件系统初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "hdfs namenode -format\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 启动hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化 HDFS\n",
    "``` \n",
    "bin/hadoop namenode -format\n",
    "```\n",
    "启动 HDFS\n",
    "```\n",
    "sbin/start-dfs.sh\n",
    "```\n",
    "启动 YARN\n",
    "```\n",
    "sbin/start-yarn.sh\n",
    "```\n",
    "直接一步启动，切换到 sbin 目录(不推荐)\n",
    "``` bash\n",
    "./start-all.sh\n",
    "```\n",
    "查看进程(java)快照\n",
    "``` bash\n",
    "jps\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置 ssh 免密码登陆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（hadoop 启停免登录）\n",
    "``` bash\n",
    "cd ~/.ssh/\n",
    "```\n",
    "\n",
    "- 生成密钥\n",
    "\n",
    "``` bash\n",
    "ssh-keygen -t rsa\n",
    "```\n",
    "\n",
    "- 拷贝公钥到已认证文件\n",
    "\n",
    "``` bash\n",
    "cp id_rsa.pub authorized_keys\n",
    "```\n",
    "\n",
    "- 或者\n",
    "\n",
    "``` bash\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "```\n",
    "\n",
    "- 拷贝公钥到其他机器\n",
    "\n",
    "``` bash\n",
    "ssh-copyid address/hostname\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试 HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 上传文件\n",
    "``` bash\n",
    "hadoop fs -put file address\n",
    "```\n",
    "\n",
    "#### 下载文件\n",
    "``` bash\n",
    "hadoop fs -get address file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS 的 shell 查看\n",
    "``` bash\n",
    "hadoop fs\n",
    "```\n",
    "或：\n",
    "``` bash\n",
    "hdfs dfs\n",
    "```\n",
    "\n",
    "查看帮助：\n",
    "``` bash \n",
    "hadoop fs -help <cmd>\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试 MapReduce/YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "hadoop jar hadoop-mapreduce-exp.jar wordcount hdfs://hostname:/words hdfs://hostname:/out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化\n",
    "\n",
    "1. 启动 ZK（4，5，6）\n",
    "``` bash\n",
    "./zkServer.sh start\n",
    "```\n",
    "\n",
    "2. 启动 journalnode（4，5，6）\n",
    "``` bash\n",
    "c /path/hadoop-2.2.0\n",
    "sbin/hadoop-daemon.sh start journalnode\n",
    "```\n",
    "\n",
    "3. 格式化 HDFS（在h1 上格式化，再拷贝tmp 到其他主机h2）\n",
    "``` bash\n",
    "hdfs namenode -format\n",
    "```\n",
    "\n",
    "4. 格式化 ZK（只需要在h1 上格式化）\n",
    "``` bash\n",
    "hdfs zkfc -formatZK\n",
    "```\n",
    "\n",
    "5. 启动／停止 HDFS（在h1上）\n",
    "``` bash\n",
    "sbin/start-dfs.sh\n",
    "sbin/stop-dfs.sh\n",
    "```\n",
    "\n",
    "6. 启动／停止 YARN（在h3）\n",
    "``` bash\n",
    "sbin/start-yarn.sh\n",
    "sbin/stop-yarn.sh\n",
    "```\n",
    "\n",
    "7. 官方文档\n",
    "``` txt\n",
    "hadoop-ver\\share\\doc\\index\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集群测试\n",
    "``` java\n",
    "public static void main(String[] args) throw Exception{\n",
    "    Configuration conf = new Configuration();\n",
    "    conf.set(\"dfs.nameservices\",\"ns1\");\n",
    "    conf.set(\"dfs.ha.namenode.ns1\",\"nn1,nn2\");\n",
    "    conf.set(\"dfs.namenode.rpc-address.ns1.nn1\",\"h1:9000\");\n",
    "    conf.set(\"dfs.namenode.rpc-address.ns1.nn2\",\"h2:9000\");\n",
    "    conf.set(\"dfs.client.failover.proxy.provider.ns1\",\"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvicer\");\n",
    "    FileSystem fs = FileSystem.get(new URI(\"hdfs:ns1\"), conf,\"root\");\n",
    "    // 上传\n",
    "    InputStream in = new FileInputStream(\"test\");\n",
    "    OutputStream out = fs.create(new Path(\"/test\"));\n",
    "    // 下载\n",
    "    // InputStream in = fs.open(new Path(\"/test\"));\n",
    "    // OutputStream out = new FileOutputStream(\"D://test\");\n",
    "    IOUtils.copyBytes(in, out, 4096, true);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分布式文件系统\n",
    "- GFS\n",
    "- HDFS\n",
    "- Lustre\n",
    "- Ceph\n",
    "- mogileFS\n",
    "- GridFS\n",
    "- TFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS 原理\n",
    "![hdfs theory](./images/hdfs-theory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS Architecture\n",
    "![hdfs architecture](./images/hdfs-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 元数据存储\n",
    "```\n",
    "NameNode Metadata\n",
    "NameNode(FileName, Replication, block-ids, idzhost...)\n",
    "/test/a.log,3,{blk_1,blk_2},[{blk_1:[h0,h1,h3]},{blk_2:[h0,h2,h4]}]\n",
    "```\n",
    "![hdfs-datanodes-data](./images/hdfs-datanodes-data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NameNode 介绍\n",
    "整个文件系统的管理节点\n",
    "- 文件目录树管理（元数据与数据块列表管理）。\n",
    "- 接收用户操作请求，执行命令。\n",
    "\n",
    "成员：\n",
    "- fsimage：元数据镜像文件（hdfs.site.xml中的 dfs.name.dir 属性），NameNode 内存元数据信息。\n",
    "- edits：操作日志文件。\n",
    "- fstime：保存最近一次checkpoint还原点的时间。\n",
    "\n",
    "以上文件是保存在Linux 文件系统的（../hadoop-2.20/tmp/dfs/name/current/）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NameNode 特点\n",
    "1. Metadata 始终保存在内存，用于处理“读请求”。\n",
    "2. “写请求”时，先向edits 写日志，成功后改内存元数据，最后向客户端返回，并写入 DataNode。\n",
    "3. fsimage 由 edits 与旧的 fsimage 同步合并生成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据存储服务\n",
    "1. 文件块 block，HDFS 默认 128 MB。\n",
    "2. 不同与普通文件系统， HDFS 中文件小于 128 MB时，并不占用整块空间\n",
    "3. Replication 多副本，默认三个（hdfs-site.xml的dfs.replication 属性）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary NameNode\n",
    "伪分布式独有，用于生成 fsimage\n",
    "\n",
    "- checkpoint 的时机\n",
    "    1. fs.checkpoint.period 间隔默认 3600s\n",
    "    2. fs.checkpoint.size edits 最大值默认 64M\n",
    "![secondary-namenode](./images/secondary-namenode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS 的 Java 接口\n",
    "通过 RPC 代理实现通行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eclipse\n",
    "1. 新建 Java Project\n",
    "2. lib 文件夹引入 hadoop.jar\n",
    "3. Build Path-> Add to buildPath\n",
    "4. 新建 class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演示\n",
    "``` java\n",
    "public class HDFSDemo{\n",
    "    FileSystem fs = null;\n",
    "    \n",
    "    @Before\n",
    "    public void init() throw IOException,URIException{\n",
    "        fs = FileSystem(new URI(\"hdfs://hostname:9000\"), new Configuration(), \"root\");\n",
    "    }\n",
    "    \n",
    "    @Test //上传\n",
    "    public void testUpload() throw Exception{\n",
    "        InputStream in = new FileInputSystem(\"c://test.txt\");\n",
    "        OutputStream out = fs.Create(new Path(\"/text\"));\n",
    "        IOUtils.copyBytes(in, out, 4096, true);\n",
    "    }\n",
    "    \n",
    "    @Test //下载\n",
    "    public void testDownload() throw Exception{\n",
    "        fs.copyToLocalFile(new Path(\"\"), new Path(\"\"));\n",
    "    }\n",
    "    \n",
    "    @Test //删除\n",
    "    public void testdel() throw Exception{\n",
    "        boolean flag = fs.delete(new Path(\"/test\", false);\n",
    "    }\n",
    "    \n",
    "    @Test //创建\n",
    "    public void testMKDir() throw Exception{\n",
    "        boolean flag = fs.mkdirs(new Path(\"\"));\n",
    "    }\n",
    "    \n",
    "    // 下载\n",
    "    public static void main(String[] args) throws ... {\n",
    "        InputStream in = fs.open(new Path(\"/test\"));\n",
    "        OutputStream out = new FileOutputStream(\"c://test.txt\");\n",
    "        IOUtils.copyBytes(in, out, 4096, true);\n",
    "    }    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPC(Remote Procedure Call)\n",
    "远程过程调用协议，底层使用 socket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义接口\n",
    "``` java\n",
    "public interface Bizable{\n",
    "    public static final long versionId = 10000;\n",
    "    public String sayHi(String name);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现\n",
    "``` java\n",
    "public class RPCServer implements Bizable{\n",
    "    public String sayHi(String name){\n",
    "        return \"hi ~\" + name;\n",
    "    }\n",
    "    \n",
    "    public static void main(String[] args) throw Exception{\n",
    "        Configuration conf = new Configuration();\n",
    "        Server server = new RPC.Builder(conf).SetProtocal(Bizable.class).SetInstance(new RPCServer()).SetBindAddress(\"ip\").SetPort(port).build();\n",
    "        server.start();\n",
    "    }\n",
    "}\n",
    "\n",
    "public class RPCClient{\n",
    "    public static void main(String[] args) throw Exception{\n",
    "        Bizable proxy = RPC.getProxy(Bizable.class, 10000, new InetSocketAddress(\"ip\", port), new Configuration());\n",
    "        String result = proxy.sayHi(\" tomcat \");\n",
    "        System.out.printIn(result);\n",
    "        RPC.stopProxy(proxy);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce \n",
    "``` \n",
    "    分布式计算模型，解决海量数据计算问题，Map() 和 Reduce() 实现分布式计算，两个函数的形参都是key，value 对。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce 框架\n",
    "![mapreduce-architecture](./images/mapreduce-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce 原理\n",
    "![mapreduce-theory](./images/mapreduce-theory.png)\n",
    "Mappers must complete before Reducers can begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce 执行过程\n",
    "#### map\n",
    "1. 读取文件，每行解析成一堆key value， 每对调用一次 map 函数\n",
    "2. map 函数生成新的 keyvalue 对\n",
    "3. 对新的 keyvalue 分区\n",
    "4. 对不同分区的数据按 key 排序分组\n",
    "5. 分组数据归约\n",
    "\n",
    "#### reduce\n",
    "1. 对多个 map 的输出按分区 copy 到不同的reduce 节点\n",
    "2. 对多个 map 的输出进行行合并排序，通过reduce 函数生成新 keyvalue对\n",
    "3. 将 reduce 的输出保存到文件中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example 1: WordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 执行流程\n",
    "|HDFS | mapper input | mapper output | recude output | \n",
    "|-|-|-|-|\n",
    "| hello tom | <0, \"hello tom\"> | <hello, {1,1,1,1}> | <hello, 4> |\n",
    "| hello jerry | <10, \"hello jerry\"> | <jerry, {1}> | <jerry, 1> |\n",
    "| hello kitty | <22, \"hello kitty\"> | <kitty,{1}> | <kitty, 1> |\n",
    "| hello world | <34, \"hello world\"> | <tom, {1}> | <tom, 1> |\n",
    "| hello tom | <46, \"hello tom\"> | <world, {1}> | <world, 1> |\n",
    "\n",
    "***\n",
    "``` java\n",
    "map(){\n",
    "    String vlaue = v1;\n",
    "    String[] words = value.split(\" \");\n",
    "    for(String w:words){\n",
    "        context.write(w, 1);\n",
    "    }\n",
    "}\n",
    "```\n",
    "``` java\n",
    "reduce(){\n",
    "    String key = k2;\n",
    "    int counter =0;\n",
    "    for(int i: v2s){\n",
    "        counter +=1;\n",
    "    }\n",
    "    context.write(key, counter);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码实现\n",
    "``` java\n",
    "public class WCMapper extends Mapper<LongWritable, Text, Test, LongWritable>{\n",
    "    @override\n",
    "    protecte void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException{\n",
    "        String line = value.toString();\n",
    "        String[] words = line.split(\" \");\n",
    "        for(String w: words){\n",
    "            context.write(new Text(w), new LongWritable(1));\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "public class WCReducer extends Reducer<Text, LongWritable, Text, LongWritable>{\n",
    "    @override\n",
    "    protected void reduce(Text key, Interable<LongWritable> v2s, Context context) throws IOException, InterruptedException{\n",
    "        long counter = 0;\n",
    "        for(LongWritable i : v2s){\n",
    "            counter += i.get();\n",
    "        }\n",
    "            context.write(key, new LongWritable(counter));\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 封装 WordCount 的 MapReduce\n",
    "``` java\n",
    "public class WordCount{\n",
    "    public static void main(String[] args) throws Exception{\n",
    "        Job job =Job.getInstance(new Configuration());\n",
    "        job.setJarByClass(WordCount.class);\n",
    "        job.setMapperClass(WCMapper.class);\n",
    "        // 当 Map 与 Reduce 的输出类型一致时\n",
    "        // 可只 setOutputValue／Key，不设 setMapOutputKey／Value\n",
    "        job.setMapOutputKeyClass(Text.class);\n",
    "        job.setMapOutputValueClass(LongWritable.class);\n",
    "        FileInputFormat.setInputPaths(job, new Path(\"...\"));\n",
    "        job.setReducerClass(WCReduce.class);\n",
    "        job.setOutputKeyClass(Text.class);\n",
    "        job.setOutputValueClass(LongWritable.class);\n",
    "        FileOutputFormat.setOutputPath(job, new Path(\"...\"));\n",
    "        // 提交任务\n",
    "        job.waitForCompletion(true);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 执行命令\n",
    "``` bash\n",
    "hadoop jar /root/exam.jar groupid.class /data/dout\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example 2: DataCount\n",
    "``` java\n",
    "public class DataCount{\n",
    "    public static void main(String[] args) throws Exception{\n",
    "        // 同1.3.6\n",
    "    }\n",
    "    \n",
    "    public static class DCMapper extents Mapper<LongWritable, Text, Text, DataBean>{\n",
    "        @override\n",
    "        protected void map(LongWritable key, Text value, Context context) throws Exception{\n",
    "            String[] fields = value.toString().split(\"\\t\");\n",
    "            DataBean bean = new DataBean(fields[1], fields[8], fields[9]);\n",
    "            context.write(new Text(fields[1], bean);\n",
    "        }\n",
    "    }\n",
    "                          \n",
    "    public static class DCReducer extends Reducer<Text, DataBean, Text, DataBean>{\n",
    "        @override\n",
    "        protected void reduce(Text key, Iterable<DataBean> vs, Context context) throws Exception{\n",
    "            long up_sum = 0, down_sum = 0;\n",
    "            for( DataBean bean:vs){\n",
    "                up_sum += bean.getUpPayLoad();\n",
    "            }\n",
    "            down_sum += bean.getDownPayLoad();\n",
    "            Context.write(key, new DataBean(key, up_sum, down_sum, up_sum+down_sum));\n",
    "        }\n",
    "    }\n",
    "}\n",
    "                          \n",
    "public class DataBean impliment Writable{\n",
    "    private String telNum;\n",
    "    private long upload;\n",
    "    ...\n",
    "        \n",
    "    @override\n",
    "    public wirte(out){\n",
    "    }\n",
    "    \n",
    "    @override\n",
    "    public read(in){\n",
    "    }\n",
    "    \n",
    "    @override\n",
    "    public String toString(){\n",
    "        return \"\";\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 远程调试\n",
    "``` bash\n",
    "# hadoop 远程 debug 配置\n",
    "# 在 ../hadoop-2.9.1/etc/hadoop/hadoop-env.sh 追加\n",
    "# 远程调试 Namenode\n",
    "export HADOOP_NAMENODE_OPTS=\"-agentlib:jdwp=transport=dt_socket,address=...,server=y,suspend=y\"\n",
    "# 远程调试 Datanode\n",
    "export HADOOP_DATANODE_OPTS=\"-agentlib:jdwp=transport=dt_socket,address=...,server=y,suspend=y\"\n",
    "# 远程调试 ResourceManager\n",
    "export HADOOP_RESOURCEMANAGER_OPTS=\"-agentlib:jdwp=transport=dt_socket,address=...,server=y,suspend=y\"\n",
    "# 远程调试 NodeManager\n",
    "export HADOOP_NODEMANAGER_OPTS=\"-agentlib:jdwp=transport=dt_socket,address=...,server=y,suspend=y\"\n",
    "```\n",
    "```\n",
    "\"-agentlib: jdwp=transportdt_sorket,address=8888, server=y, suspend=y\"\n",
    "参数说明：\n",
    "启动 JDWP 实现， dt_socket 套接字\n",
    "JVM 在 8888 端口监听请求\n",
    "server=y 表示 JVM 是被调试者，n 表示调试器\n",
    "suspend=y 表示会被挂起\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioner\n",
    "#### example\n",
    "``` java\n",
    "public static class ProviderPartitioner extends Partitioner<Text, DataBean>{\n",
    "    @override\n",
    "    public int getPartition(Text key, DataBean bean, int partitions){\n",
    "        String sub_acc = key.toString.subString(0, 3);\n",
    "        Integer code = providerMap.get(sub_acc);\n",
    "        if(code==null){\n",
    "            code=0;\n",
    "        }\n",
    "        return code;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "``` java\n",
    "//main\n",
    "job.setPartitionerClass(ProviderPartitioner.Class);\n",
    "// 决定 Reducer 数和 Partitioner 数，也就是结果数\n",
    "// Reducer 数必须大于等于 Partitioner 数\n",
    "job.setNumReduceTask(args[2]);\n",
    "job.waitForcompletion(true);\n",
    "```\n",
    "\n",
    "#### sort\n",
    "```\n",
    "通过 tocompare 重载，将要排序的对象作为 Reducer 的 key, 可以实现排序\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combiner\n",
    "\n",
    "```\n",
    "可插拔类的 Combiner\n",
    "只是可以提效，不改变结果\n",
    "    在 map 端对输出先做一次合并，以减少传输到Reducer 的数据量，相对提升效率；Combiner 只应用于 Reduce 的输入和输出类型完全一致的场景，比如累加，最大值等\n",
    "```\n",
    "``` java\n",
    "// 直接是用 Reducer 的实现\n",
    "job.setCombinerClass(WCReducer.Class);\n",
    "```\n",
    "|HDFS | mapper | combiner |\n",
    "|-|-|-|\n",
    "| hello tom | <hello, {1,1,1}> | <hello, 3> |\n",
    "| hello tom | <tom, {1,1}> | <tom, 2> |\n",
    "| hello kitty | <kitty,{1}> | <kitty, 1> |\n",
    "|\n",
    "| hello tom | <hello, {1,1}> | <hello,2> |\n",
    "| hello tom | <tom, {1}> | <tom, 2> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle\n",
    "\n",
    "分区、排序、Combiner、分组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example 3: 倒排索引\n",
    "```\n",
    "    倒排索引，也常被称为反向索引，置入档案或反向档案，是一种索引方式，被用来存储在全文搜索下某个单词或者一组文档中的存储位置的映射。\n",
    "    它是文档系统中最常用的数据结构。通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。\n",
    "    倒排索引主要有两部分组成：“单词词典”和“倒排文件”\n",
    "```\n",
    "\n",
    "伪代码\n",
    "``` java\n",
    "// mapper\n",
    "context.write(\"hello, a.txt\", 1);\n",
    "context.write(\"hello, a.txt\", 1);\n",
    "context.write(\"hello, b.txt\", 1);\n",
    "```\n",
    "***\n",
    "``` java\n",
    "// combiner\n",
    "context.write(\"hello, a.txt\", 2);\n",
    "context.write(\"hello, b.txt\", 1);\n",
    "```\n",
    "***\n",
    "``` java\n",
    "// reducer 1\n",
    "context.write(\"hello\", \"a.txt 2\");\n",
    "context.write(\"hello\", \"b.txt 1\");\n",
    "```\n",
    "***\n",
    "```\n",
    "空 mapper\n",
    "```\n",
    "***\n",
    "``` java\n",
    "// reducer 2\n",
    "context.write(\"hello\", {\"a.txt, 2\",\"b.txt, 1\"});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapper 数量因素\n",
    "```\n",
    "split 大小 = Math.max(minSize, Math.min(maxSize, blockSize));\n",
    "mapper 数 = split结果 block 块数\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
